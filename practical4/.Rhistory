# offsets. Update the network with lists dh, dW, and db.
backward <- function(nn, k) {
#get the number of layers
n_layers <- length(nn$h)
#get the number of data points in output class k
n_data <- length(k)
#learning rate(given) for gradient descent
eta <- 0.01
#function to compute derivatives for the output layer
output_layer_derivative <- function(h, k) {
exp_h <- exp(h)
#compute pk
pk <- exp_h / sum(exp_h)
#update pk of true class
pk[k] <- pk[k] - 1
return(pk)
}
#function to compute derivatives for hidden layers
backward_output_layer <- function(nn, k, updated_network) {
n_layers <- length(nn$h)
#compute derivatives for the output layer
nn$dh[[n_layers]] <- output_layer_derivative(updated_network$h[[n_layers]], k)
#back-propagation to compute derivatives for hidden layers
for (l in rev(seq_len(n_layers - 1))) {
#compute derivatives for the current hidden layer
nn$dh[[l]] <- output_layer_derivative(updated_network$h[[l]], k)
#compute derivatives for weights and offsets
nn$dW[[l]] <- nn$dh[[l + 1]] %*% t(nn$h[[l]])
nn$db[[l]] <- nn$dh[[l + 1]]
#ensure that nn$h[[l]] is a column vector for matrix multiplication
nn$h[[l]] <- matrix(nn$h[[l]], ncol = 1)
#update nn$dh[[l]] using matrix multiplication
nn$dh[[l]] <- ifelse(nn$h[[l]] > 0, t(nn$W[[l]]) %*% nn$dh[[l + 1]], 0)
}
return(nn)
}
#initialize lists for derivatives
nn$dh <- vector("list", length = n_layers)
nn$dW <- vector("list", length = n_layers - 1)
nn$db <- vector("list", length = n_layers - 1)
#initialize loss (and thereby, average loss)
total_loss <- 0
#compute derivatives for the output layer and accumulate loss
#loop through each data point
for (i in 1:n_data) {
#forward pass to compute the network's output for the current data point.
updated_network <- forward(nn, inp[i, ])
#compute the pk for the output layer(loss for data point i)
pk <- exp(updated_network$h[[n_layers]]) / sum(exp(updated_network$h[[n_layers]]))
#negative log-likelihood loss for the current data point and class
total_loss <- total_loss - log(pk[k[i]])
#backward pass to compute derivatives and update the network's gradients.
nn <- backward_output_layer(nn, k[i], updated_network)
#loop through each layer's weights and offsets
for (l in seq_along(nn$W)) {
nn$dW[[l]] <- nn$dW[[l]] + updated_network$dW[[l]]
nn$db[[l]] <- nn$db[[l]] + updated_network$db[[l]]
}
}
#calculate average loss
average_loss <- total_loss / n_data
#update network parameters using the average gradients, looping through each layer
for (l in seq_along(nn$W)) {
nn$W[[l]] <- nn$W[[l]] - eta * (nn$dW[[l]] / n_data)
nn$b[[l]] <- nn$b[[l]] - eta * (nn$db[[l]] / n_data)
}
return(nn)
}
# ------- Step4 ------- #
# Implement a training function train(nn, inp, k, eta, mb, nstep)
# to train the neural network using stochastic gradient descent.
# It should take input data (inp), labels (k), learning rate (eta),
# mini-batch size (mb), and the number of optimization steps (nstep).
train <- function(nn, inp, k, eta = 0.01, mb = 10, nstep = 10000) {
#number of data points in training set
n_data <- nrow(inp)
#function to compute derivatives for hidden layers
backward_output_layer <- function(nn, k, updated_network) {
n_layers <- length(nn$h)
#compute derivatives for the output layer
nn$dh[[n_layers]] <- output_layer_derivative(updated_network$h[[n_layers]], k)
#back-propagation to compute derivatives for hidden layers
for (l in rev(seq_len(n_layers - 1))) {
#compute derivatives for the current hidden layer
nn$dh[[l]] <- output_layer_derivative(updated_network$h[[l]], k)
#compute derivatives for weights and offsets
nn$dW[[l]] <- nn$dh[[l + 1]] %*% t(nn$h[[l]])
nn$db[[l]] <- nn$dh[[l + 1]]
#ensure that nn$h[[l]] is a column vector for matrix multiplication
nn$h[[l]] <- matrix(nn$h[[l]], ncol = 1)
#update nn$dh[[l]] using matrix multiplication
nn$dh[[l]] <- ifelse(nn$h[[l]] > 0, t(nn$W[[l]]) %*% nn$dh[[l + 1]], 0)
}
return(nn)
}
#function to compute derivatives for the output layer
output_layer_derivative <- function(h, k) {
exp_h <- exp(h)
#compute pk
pk <- exp_h / sum(exp_h)
#update pk of true class
pk[k] <- pk[k] - 1
return(pk)
}
#initialize gradients for the mini-batch using lapply
#create a matrix of zeros with the same dimensions as the
#corresponding weight matrix in nn$W
mini_batch_dW <- lapply(nn$W, function(x) matrix(0, nrow = nrow(x), ncol = ncol(x)))
#create a numeric vector of zeros with the same length as the
#corresponding offset vector in nn$b
mini_batch_db <- lapply(nn$b, function(x) numeric(length(x)))
#training steps
for (step in 1:nstep) {
#randomly sample mb indices for mini-batch
indices <- sample(1:n_data, mb, replace = TRUE)
#compute gradients and loss for the mini-batch
for (i in indices) {
#initialize loss
total_loss <- 0
#forward pass
updated_network <- forward(nn, inp[i,])
#compute loss for the data point i
pk <- exp(updated_network$h[[length(updated_network$h)]]) /
sum(exp(updated_network$h[[length(updated_network$h)]]))
total_loss <- total_loss - log(pk[k[i]])
#backward pass for the corresponding class k
nn <- backward_output_layer(nn, k[i], updated_network)
#accumulate gradients for the data point i
for (l in seq_along(nn$W)) {
nn$dW <- lapply(nn$W, function(x) matrix(0, nrow = as.numeric(nrow(x)),
ncol = as.numeric(ncol(x))))
nn$db <- lapply(nn$b, function(x) numeric(length(x)))
}
}
#update network parameters using the average gradients
for (l in seq_along(nn$W)) {
nn$W[[l]] <- nn$W[[l]] - eta * (nn$dW[[l]] / mb)
nn$b[[l]] <- nn$b[[l]] - eta * (nn$db[[l]] / mb)
}
#print average loss for monitoring training progress(every 200 steps)
if (step %% 200 == 0) {
cat("Step:", step, "   Average Loss:", total_loss / mb, "\n")
}
}
return(nn)
}
# ------- Step5 ------- #
# We load the iris dataset, shuffle it, and split it into training
# and test sets. Train a neural network with layer dimensions
# (4-8-7-3).
# loading the iris data from base R
data(iris)
# Setting a seed for reproducibility
set.seed(69)
# Shuffling the dataset using sample function
iris <- iris[sample(nrow(iris)), ]
# Splitting the data into training and test sets
# The number of samples for the training data are 85% of the
# total dataset, for which we use the floor function.
n_train <- floor(0.85 * nrow(iris))
# Extracting training data and labels based on the logic that
# test data consists of every 5th row and training data becomes
# the remaining data.
# Excluding the 5th column, which contains the class labels
train_data <- iris[1:n_train, -5]
# Extracting the class labels as integers
train_labels <- as.integer(iris[1:n_train, 5])
# Extracting test data and labels
test_data <- iris[(n_train + 1):nrow(iris), -5]
test_labels <- as.integer(iris[(n_train + 1):nrow(iris), 5])
# Training a 4-8-7-3 network using the netup and train function
nn <- netup(c(4, 8, 7, 3))
nn <- train(nn, train_data, train_labels)
# ------- Step6 ------- #
# Writing a function to predict class labels for the test set
# using the trained neural network. Computing the misclassification
# rate for the test set.
# Generating a function to classify the test data
# into the predicted class.
predict_class <- function(nn, inp) {
# Forward pass for neural network
forward_pass <- forward(nn, inp)
# Find the index of the class with the highest probability
predicted_class <- which.max(forward_pass$h[[length(nn$h)]])
return(predicted_class)
}
# Function to compute the misclassification rate, which is the
# proportion of misclassificed samples in the dataset.
compute_misclassification_rate <- function(nn, test_data, test_labels) {
n_test <- nrow(test_data)
# first we set misclassified rate as 0
misclassified <- 0
for (i in 1:n_test) {
# Predict the class label for each test instance
# using the predict_class function defined above
predicted_class <- predict_class(nn, test_data[i, ])
# Check if the predicted class is different from the
# true class by comparing
if (predicted_class != test_labels[i]) {
misclassified <- misclassified + 1
}
}
# Computing misclassification rate
misclassification_rate <- misclassified / n_test
return(misclassification_rate)
}
# Using the trained neural network to predict class labels for the test set
misclassification_rate <- compute_misclassification_rate(nn, test_data, test_labels)
# Print the misclassification rate
cat("Misclassification Rate:", misclassification_rate, "\n")
# ------- Conclusion of the project ------- #
# In conclusion, the provided code implements a simple fully
# connected neural network for a classification task using the
# ReLU activation function and stochastic gradient descent (SGD)
# for optimization. The steps involve initializing the neural
# network, performing forward and backward passes, training the
# network using SGD, and evaluating its performance on a test set.
# Thank you for going through our project.
#################
#   THANK YOU   #
#################
#--------------------#
#    Practical 4     #
#--------------------#
# ------- Group Introduction ------- #
# The following members form Group 27:
#   1. Anagha - S2596158
#   2. Anjali - S2580177
#   3. Navya  - S2602687
#Address of git repo:- https://github.com/ntxpoppy/GROUP-27
# Contribution of each member:
# We collectively decided the distribution of questions.
#   1. Anagha - 34% - Step3 and Step4
#   2. Anjali - 33% - Step5 and Step6
#   3. Navya  - 33% - Step1 and Step2
# ------- Overview of the project ------- #
# The project involves implementing a simple fully connected
# neural network for a classification task using the ReLU
# activation function and stochastic gradient descent (SGD)
# for optimization. The steps required are:
# Step1: Network Initialization
# Step2: Forward Pass
# Step3: Backward Pass
# Step4: Training Function
# Step5: Data Preprocessing and Training
# Step6: Evaluation
# ------- Step1 ------- #
# We write a function netup that takes a vector d representing
# the number of nodes in each layer and returns a list
# representing the neural network. This list should contain
# elements for nodes (h), weight matrices (W), and offset vectors
# (b). We initialize weights and offsets with random deviates.
netup <- function(d) {
# Initialize a list to store neural network components
nn <- list()
# Initialize node values 'h' as a list with the same length as 'd'
nn$h <- vector("list", length = length(d))
# Initialize node values for each layer
for (l in 1:length(d)) {
nn$h[[l]] <- numeric(d[l])
}
# Initialize weights matrices 'W' and offset parameter matrices 'b'
nn$W <- vector("list", length = length(d) - 1)
nn$b <- vector("list", length = length(d) - 1)
# Iterate through layers to initialize weights and offsets
for (l in 1:(length(d) - 1)) {
nn$W[[l]] <- matrix(runif(d[l+1] * d[l], 0, 0.2),
nrow = d[l+1], ncol = d[l])
nn$b[[l]] <- runif(d[l + 1], 0, 0.2)
}
# Return the initialized neural network
return(nn)
}
# ------- Step2 ------- #
# Implement a function forward(nn, inp) that takes a neural
# network and input values for the first layer (inp).
# It computes the node values for each layer using the
# ReLU activation function and returns the updated network.
forward <- function(nn, inp=nn$h[[1]]) {
#iterating through the weights for layers
for (l in seq_along(nn$W)) {
#forward pass for each layer using the ReLU activation function
nn$h[[l + 1]] <- pmax(0, nn$W[[l]]%*%nn$h[[l]] + nn$b[[l]])
}
#return the updated neural network
return(nn)
}
# ------- Step3 ------- #
# Here we write a function backward(nn, k) to compute the derivatives
# of the loss corresponding to the output class k. This involves
# back-propagation to compute derivatives for nodes, weights, and
# offsets. Update the network with lists dh, dW, and db.
backward <- function(nn, k) {
#get the number of layers
n_layers <- length(nn$h)
#get the number of data points in output class k
n_data <- length(k)
#learning rate(given) for gradient descent
eta <- 0.01
#function to compute derivatives for the output layer
output_layer_derivative <- function(h, k) {
exp_h <- exp(h)
#compute pk
pk <- exp_h / sum(exp_h)
#update pk of true class
pk[k] <- pk[k] - 1
return(pk)
}
#function to compute derivatives for hidden layers
backward_output_layer <- function(nn, k, updated_network) {
n_layers <- length(nn$h)
#compute derivatives for the output layer
nn$dh[[n_layers]] <- output_layer_derivative(updated_network$h[[n_layers]], k)
#back-propagation to compute derivatives for hidden layers
for (l in rev(seq_len(n_layers - 1))) {
#compute derivatives for the current hidden layer
nn$dh[[l]] <- output_layer_derivative(updated_network$h[[l]], k)
#compute derivatives for weights and offsets
nn$dW[[l]] <- nn$dh[[l + 1]] %*% t(nn$h[[l]])
nn$db[[l]] <- nn$dh[[l + 1]]
#ensure that nn$h[[l]] is a column vector for matrix multiplication
nn$h[[l]] <- matrix(nn$h[[l]], ncol = 1)
#update nn$dh[[l]] using matrix multiplication
nn$dh[[l]] <- ifelse(nn$h[[l]] > 0, t(nn$W[[l]]) %*% nn$dh[[l + 1]], 0)
}
return(nn)
}
#initialize lists for derivatives
nn$dh <- vector("list", length = n_layers)
nn$dW <- vector("list", length = n_layers - 1)
nn$db <- vector("list", length = n_layers - 1)
#initialize loss (and thereby, average loss)
total_loss <- 0
#compute derivatives for the output layer and accumulate loss
#loop through each data point
for (i in 1:n_data) {
#forward pass to compute the network's output for the current data point.
updated_network <- forward(nn, inp[i, ])
#compute the pk for the output layer(loss for data point i)
pk <- exp(updated_network$h[[n_layers]]) / sum(exp(updated_network$h[[n_layers]]))
#negative log-likelihood loss for the current data point and class
total_loss <- total_loss - log(pk[k[i]])
#backward pass to compute derivatives and update the network's gradients.
nn <- backward_output_layer(nn, k[i], updated_network)
#loop through each layer's weights and offsets
for (l in seq_along(nn$W)) {
nn$dW[[l]] <- nn$dW[[l]] + updated_network$dW[[l]]
nn$db[[l]] <- nn$db[[l]] + updated_network$db[[l]]
}
}
#calculate average loss
average_loss <- total_loss / n_data
#update network parameters using the average gradients, looping through each layer
for (l in seq_along(nn$W)) {
nn$W[[l]] <- nn$W[[l]] - eta * (nn$dW[[l]] / n_data)
nn$b[[l]] <- nn$b[[l]] - eta * (nn$db[[l]] / n_data)
}
return(nn)
}
# ------- Step4 ------- #
# Implement a training function train(nn, inp, k, eta, mb, nstep)
# to train the neural network using stochastic gradient descent.
# It should take input data (inp), labels (k), learning rate (eta),
# mini-batch size (mb), and the number of optimization steps (nstep).
train <- function(nn, inp, k, eta = 0.01, mb = 10, nstep = 10000) {
#number of data points in training set
n_data <- nrow(inp)
#function to compute derivatives for hidden layers
backward_output_layer <- function(nn, k, updated_network) {
n_layers <- length(nn$h)
#compute derivatives for the output layer
nn$dh[[n_layers]] <- output_layer_derivative(updated_network$h[[n_layers]], k)
#back-propagation to compute derivatives for hidden layers
for (l in rev(seq_len(n_layers - 1))) {
#compute derivatives for the current hidden layer
nn$dh[[l]] <- output_layer_derivative(updated_network$h[[l]], k)
#compute derivatives for weights and offsets
nn$dW[[l]] <- nn$dh[[l + 1]] %*% t(nn$h[[l]])
nn$db[[l]] <- nn$dh[[l + 1]]
#ensure that nn$h[[l]] is a column vector for matrix multiplication
nn$h[[l]] <- matrix(nn$h[[l]], ncol = 1)
#update nn$dh[[l]] using matrix multiplication
nn$dh[[l]] <- ifelse(nn$h[[l]] > 0, t(nn$W[[l]]) %*% nn$dh[[l + 1]], 0)
}
return(nn)
}
#function to compute derivatives for the output layer
output_layer_derivative <- function(h, k) {
exp_h <- exp(h)
#compute pk
pk <- exp_h / sum(exp_h)
#update pk of true class
pk[k] <- pk[k] - 1
return(pk)
}
#initialize gradients for the mini-batch using lapply
#create a matrix of zeros with the same dimensions as the
#corresponding weight matrix in nn$W
mini_batch_dW <- lapply(nn$W, function(x) matrix(0, nrow = nrow(x), ncol = ncol(x)))
#create a numeric vector of zeros with the same length as the
#corresponding offset vector in nn$b
mini_batch_db <- lapply(nn$b, function(x) numeric(length(x)))
#training steps
for (step in 1:nstep) {
#randomly sample mb indices for mini-batch
indices <- sample(1:n_data, mb, replace = TRUE)
#compute gradients and loss for the mini-batch
for (i in indices) {
#initialize loss
total_loss <- 0
#forward pass
updated_network <- forward(nn, inp[i,])
#compute loss for the data point i
pk <- exp(updated_network$h[[length(updated_network$h)]]) /
sum(exp(updated_network$h[[length(updated_network$h)]]))
total_loss <- total_loss - log(pk[k[i]])
#backward pass for the corresponding class k
nn <- backward_output_layer(nn, k[i], updated_network)
#accumulate gradients for the data point i
for (l in seq_along(nn$W)) {
nn$dW <- lapply(nn$W, function(x) matrix(0, nrow = as.numeric(nrow(x)),
ncol = as.numeric(ncol(x))))
nn$db <- lapply(nn$b, function(x) numeric(length(x)))
}
}
#update network parameters using the average gradients
for (l in seq_along(nn$W)) {
nn$W[[l]] <- nn$W[[l]] - eta * (nn$dW[[l]] / mb)
nn$b[[l]] <- nn$b[[l]] - eta * (nn$db[[l]] / mb)
}
#print average loss for monitoring training progress(every 200 steps)
if (step %% 200 == 0) {
cat("Step:", step, "   Average Loss:", total_loss / mb, "\n")
}
}
return(nn)
}
# ------- Step5 ------- #
# We load the iris dataset, shuffle it, and split it into training
# and test sets. Train a neural network with layer dimensions
# (4-8-7-3).
# loading the iris data from base R
data(iris)
# Setting a seed for reproducibility
set.seed(69)
# Shuffling the dataset using sample function
iris <- iris[sample(nrow(iris)), ]
# Splitting the data into training and test sets
# The number of samples for the training data are 85% of the
# total dataset, for which we use the floor function.
n_train <- floor(0.85 * nrow(iris))
# Extracting training data and labels based on the logic that
# test data consists of every 5th row and training data becomes
# the remaining data.
# Excluding the 5th column, which contains the class labels
train_data <- iris[1:n_train, -5]
# Extracting the class labels as integers
train_labels <- as.integer(iris[1:n_train, 5])
# Extracting test data and labels
test_data <- iris[(n_train + 1):nrow(iris), -5]
test_labels <- as.integer(iris[(n_train + 1):nrow(iris), 5])
# Training a 4-8-7-3 network using the netup and train function
nn <- netup(c(4, 8, 7, 3))
nn <- train(nn, train_data, train_labels)
# ------- Step6 ------- #
# Writing a function to predict class labels for the test set
# using the trained neural network. Computing the misclassification
# rate for the test set.
# Generating a function to classify the test data
# into the predicted class.
predict_class <- function(nn, inp) {
# Forward pass for neural network
forward_pass <- forward(nn, inp)
# Find the index of the class with the highest probability
predicted_class <- which.max(forward_pass$h[[length(nn$h)]])
return(predicted_class)
}
# Function to compute the misclassification rate, which is the
# proportion of misclassificed samples in the dataset.
compute_misclassification_rate <- function(nn, test_data, test_labels) {
n_test <- nrow(test_data)
# first we set misclassified rate as 0
misclassified <- 0
for (i in 1:n_test) {
# Predict the class label for each test instance
# using the predict_class function defined above
predicted_class <- predict_class(nn, test_data[i, ])
# Check if the predicted class is different from the
# true class by comparing
if (predicted_class != test_labels[i]) {
misclassified <- misclassified + 1
}
}
# Computing misclassification rate
misclassification_rate <- misclassified / n_test
return(misclassification_rate)
}
# Using the trained neural network to predict class labels for the test set
misclassification_rate <- compute_misclassification_rate(nn, test_data, test_labels)
# Print the misclassification rate
cat("Misclassification Rate:", misclassification_rate, "\n")
# ------- Conclusion of the project ------- #
# In conclusion, the provided code implements a simple fully
# connected neural network for a classification task using the
# ReLU activation function and stochastic gradient descent (SGD)
# for optimization. The steps involve initializing the neural
# network, performing forward and backward passes, training the
# network using SGD, and evaluating its performance on a test set.
# Thank you for going through our project.
#################
#   THANK YOU   #
#################
